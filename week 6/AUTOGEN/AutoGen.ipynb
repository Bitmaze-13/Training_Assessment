{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f85ba0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"D:\\GenAI_Langchain\\\\tchat\\.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfa73963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:413: UserWarning: Missing required field 'structured_output' in ModelInfo. This field will be required in a future version of AutoGen.\n",
      "  validate_model_info(self._model_info)\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent,UserProxyAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "import os\n",
    "\n",
    "model_info = {\n",
    "    \"vision\": True,\n",
    "    \"function_calling\": True,\n",
    "    \"json_output\": True,\n",
    "    \"family\": \"unknown\"\n",
    "}\n",
    "\n",
    "llm = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    model_info=model_info\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "052f178c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neelkanth.aher\\AppData\\Local\\Temp\\ipykernel_11152\\973782931.py:12: UserWarning: The current event loop policy is not WindowsProactorEventLoopPolicy. This may cause issues with subprocesses. Try setting the event loop policy to WindowsProactorEventLoopPolicy. For example: `asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())`. See https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop.\n",
      "  code_executor=LocalCommandLineCodeExecutor(work_dir=Path.cwd() / \"runs\")\n"
     ]
    }
   ],
   "source": [
    "from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n",
    "from pathlib import Path\n",
    "assistant = AssistantAgent(\n",
    "    name=\"Assistant\",\n",
    "    model_client=llm,\n",
    "    description=\"You're an Senior AI engineer who only give runnable python code.\"\n",
    ")\n",
    "\n",
    "code_executor = CodeExecutorAgent(\n",
    "    name= \"ExecutorAgent\",\n",
    "    model_client=llm,\n",
    "    code_executor=LocalCommandLineCodeExecutor(work_dir=Path.cwd() / \"runs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b38645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc38c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "termination = TextMentionTermination(\n",
    "    \"exit\", sources=[\"user\"]\n",
    ")\n",
    "\n",
    "user = UserProxyAgent(name=\"user\")\n",
    "\n",
    "team = RoundRobinGroupChat(\n",
    "    participants=[user, assistant, code_executor],\n",
    "    termination_condition=termination\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b96c640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for Assistant_1126ae5c-3ce5-4f28-be1b-a7284bb7ccf4/1126ae5c-3ce5-4f28-be1b-a7284bb7ccf4\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 604, in _on_message\n",
      "    return await agent.on_message(\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 79, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 827, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 955, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 624, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n",
      "    return await self._post(\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\openai\\_base_client.py\", line 1881, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\openai\\_base_client.py\", line 1666, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404\n",
      "Error processing publish message for ExecutorAgent_1126ae5c-3ce5-4f28-be1b-a7284bb7ccf4/1126ae5c-3ce5-4f28-be1b-a7284bb7ccf4\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 604, in _on_message\n",
      "    return await agent.on_message(\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NotFoundError: Error code: 404\nTraceback:\nTraceback (most recent call last):\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 827, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 955, in _call_llm\n    model_result = await model_client.create(\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 624, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n    return await self._post(\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\openai\\_base_client.py\", line 1881, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\openai\\_base_client.py\", line 1666, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.NotFoundError: Error code: 404\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m team\u001b[38;5;241m.\u001b[39mrun(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a python script to calculate the first 10 numbers of the Fibonacci sequence and print them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:307\u001b[0m, in \u001b[0;36mBaseGroupChat.run\u001b[1;34m(self, task, cancellation_token)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run the team and return the result. The base implementation uses\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m:meth:`run_stream` to run the team and then returns the final result.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03mOnce the team is stopped, the termination condition is reset.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    306\u001b[0m result: TaskResult \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_stream(\n\u001b[0;32m    308\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    309\u001b[0m     cancellation_token\u001b[38;5;241m=\u001b[39mcancellation_token,\n\u001b[0;32m    310\u001b[0m ):\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[0;32m    312\u001b[0m         result \u001b[38;5;241m=\u001b[39m message\n",
      "File \u001b[1;32md:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:518\u001b[0m, in \u001b[0;36mBaseGroupChat.run_stream\u001b[1;34m(self, task, cancellation_token)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m message\u001b[38;5;241m.\u001b[39merror \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message\u001b[38;5;241m.\u001b[39merror))\n\u001b[0;32m    519\u001b[0m     stop_reason \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: NotFoundError: Error code: 404\nTraceback:\nTraceback (most recent call last):\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 827, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 955, in _call_llm\n    model_result = await model_client.create(\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 624, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2678, in create\n    return await self._post(\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\openai\\_base_client.py\", line 1881, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n\n  File \"d:\\GenAI_Langchain\\genai_venv\\lib\\site-packages\\openai\\_base_client.py\", line 1666, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.NotFoundError: Error code: 404\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "await team.run(task=\"Write a python script to calculate the first 10 numbers of the Fibonacci sequence and print them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3bddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
