# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10jqjuzJSG9BsgnGpfjdGlYNrdZnhh-0E
"""

!pip install transformers langchain langchain-community torch huggingface_hub

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "Qwen/Qwen2.5-Coder-7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

print(f"âœ… Successfully loaded model: {model_name}")

from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_community.llms import HuggingFacePipeline
from transformers import pipeline

hf_pipeline = pipeline(
    "text-generation",
    model= model,
    tokenizer=tokenizer,
    max_new_tokens=150,
    do_sample=True,
    temperature = 0.7
)

llm = HuggingFacePipeline(pipeline = hf_pipeline)

prompt_name = PromptTemplate.from_template(
    "Suggest a fancy name for an {cuisine} restaurant. Return 'only' the name. "
)

prompt_items = PromptTemplate(
    input_varibles = ["restaurant_name"],
    template="For a restaurant named '{restaurant_name}', suggest 10 menu items on a new line each. Return *only* the items." # Simplified prompt
)

Chain = (
    {"restaurant_name": RunnablePassthrough() | prompt_name | llm}
    | RunnableParallel(
        restaurant_name=RunnablePassthrough(),
        menu_items=prompt_items | llm
    )
)

cuisine_input = "India"

output = Chain.invoke(cuisine_input)

restaurant_name_output = output.get('restaurant_name',{}).get('restaurant_name','N/A')

if ":" in restaurant_name_output:
    restaurant_name = restaurant_name_output.split(":")[-1].strip()
    # Further cleaning to remove potential extra examples or text
    restaurant_name = restaurant_name.split("Example:")[0].strip()
else:
    restaurant_name = restaurant_name_output.strip()


menu_items_output = output.get('menu_items','N/A')

if ":" in menu_items_output:
  menu_items = menu_items_output.split(":")[-1].strip()

  menu_items = menu_items.split("Example:")[0].strip()
else:
   menu_items = menu_items_output.strip()

print("Restaurant Name:")
print(restaurant_name)
print("\nMenu Items:")
print(menu_items)

template = """
You are an expert educator. Create a {num_questions} question multiple-choice quiz about {topic}.
Format each question as follows:
Question: [The question]
A) [Option]
B) [Option]
C) [Option]
D) [Option]
Correct Answer: [Letter]
"""

prompt = PromptTemplate(input_variables=["num_questions", "topic"], template=template)

# 4. Initialize the Chain
quiz_chain = prompt | llm

# 5. Generate a Quiz
topic = "Python Basics for Beginners"
num_questions = 3
response = quiz_chain.invoke({"topic": topic, "num_questions": num_questions})

print(response)

