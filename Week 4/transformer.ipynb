{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4ada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python Programs\\DevEnv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Python Programs\\DevEnv\\lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Neelkanth.aher\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 104/104 [00:00<00:00, 323.77it/s, Materializing param=pre_classifier.weight]                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9710332751274109}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "\n",
    "    model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "\n",
    "text = \"hattt saale pyaara\"\n",
    "\n",
    "result = classifier(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab689e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.554534375667572}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"chal jhutaaaa!!!!!\"\n",
    "\n",
    "result = classifier(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "263e177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python Programs\\DevEnv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Python Programs\\DevEnv\\lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Neelkanth.aher\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 382.13it/s, Materializing param=classifier.weight]                                      \n",
      "BertForTokenClassification LOAD REPORT from: dslim/bert-base-NER\n",
      "Key                      | Status     |  | \n",
      "-------------------------+------------+--+-\n",
      "bert.pooler.dense.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "Ner = pipeline(\n",
    "    \"ner\",\n",
    "    model =\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915b1648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ryker Fields -> ORG -> 0.9234036803245544\n",
      "West Europe -> LOC -> 0.9953875541687012\n"
     ]
    }
   ],
   "source": [
    "text = \"Ryker Fields co-op ltd are one of the most booming companies of West Europe\"\n",
    "\n",
    "result = Ner(text)\n",
    "\n",
    "for entity in result:\n",
    "    print(f'{entity[\"word\"]} -> {entity[\"entity_group\"]} -> {entity[\"score\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004472a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0278eaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 201/201 [00:00<00:00, 304.90it/s, Materializing param=model.norm.weight]                              \n",
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens', 'temperature'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "\n",
    "    model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "\n",
    "    max_new_tokens=200,\n",
    "\n",
    "    temperature=0.6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61e1b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain Ml in 2 sentences: \n",
      "\n",
      "Ml is a machine learning algorithm that is used to train models for classification, regression, and clustering. It is commonly used in computer vision and natural language processing applications. In the context of machine learning, Ml is a subset of machine learning algorithms that focuses on solving classification, regression, and clustering problems using data. Its main advantage is that it is a scalable algorithm that can handle large datasets easily. Ml algorithms are trained on large datasets, and once trained, they can be applied to new datasets without needing to retrain the model. This makes it faster and more efficient than traditional machine learning algorithms. Overall, Ml is a powerful tool for data scientists and machine learning enthusiasts because it allows them to model complex data and solve real-world problems.\n"
     ]
    }
   ],
   "source": [
    "result = generator(\"Explain Ml in 2 sentences\")\n",
    "\n",
    "output = result[0][\"generated_text\"]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c4495d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python Programs\\DevEnv\\lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Neelkanth.aher\\.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 371.73it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForQuestionAnswering LOAD REPORT from: deepset/roberta-base-squad2\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "Question_answering = pipeline(\n",
    "    \"question-answering\",\n",
    "\n",
    "    model=\"deepset/roberta-base-squad2\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34df6203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936\n"
     ]
    }
   ],
   "source": [
    "context = (\n",
    "    \"Harbinger System Private Limited one of the best It Consulating firm  established in 1936\"\n",
    "    \"it has ip in e-learning , Hr tech and Products used by companies across the world.\"\n",
    ")\n",
    "\n",
    "question = \"when was harbinger established ?\"\n",
    "\n",
    "result = Question_answering(question=question, context=context)\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0cd24db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python Programs\\DevEnv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7078ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\n",
    "    'csv',\n",
    "    data_files=\"Complaints.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2cd7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01fd1232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = data.map(\n",
    "    lambda c : tokens(\n",
    "        c[\"text\"],\n",
    "        padding= 'max_length',\n",
    "        truncation = True\n",
    "    ),\n",
    "    batched=True\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71cc65af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 363.78it/s, Materializing param=bert.pooler.dense.weight]                               \n",
      "BertForSequenceClassification LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a7b5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer =Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset[\"train\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44875f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python Programs\\DevEnv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:07<00:00,  7.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=1.0567872524261475, metrics={'train_runtime': 105.4583, 'train_samples_per_second': 0.256, 'train_steps_per_second': 0.057, 'total_flos': 7104062278656.0, 'train_loss': 1.0567872524261475, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd81c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  transformers  import  pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98597719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label ={\n",
    "    0: \"NEGATIVE\",\n",
    "    1: \"NEUTRAL\",\n",
    "    2: \"POSITIVE\"\n",
    "}\n",
    "\n",
    "model.config.label2id = {\n",
    "    \"NEGATIVE\" : 0,\n",
    "    \"NEUTRAL\" : 1,\n",
    "    \"POSITIVE\" : 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55e13f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab88a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DevEnv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
