{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.random.seed(0)\n",
    "data = {\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [0, 0]\n",
    "}\n",
    "labels = [0, 0, 1, 0]  # AND gate labels\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.weights_input = np.random.rand(2, 2)\n",
    "        self.weights_output = np.random.rand(2, 1)\n",
    "        self.bias_hidden = np.random.rand(1, 2)\n",
    "        self.bias_output = np.random.rand(1, 1)\n",
    "        self.learning_rate = 0.1    \n",
    "\n",
    "        def sigmoid(self, x):\n",
    "            return 1 / 1 + np.exp(-x)\n",
    "        \n",
    "        def sigmoid_derivative(self, x):\n",
    "            return x * (1 - x)\n",
    "        \n",
    "        def error(self,y,y_p):\n",
    "            return y - y_p\n",
    "        \n",
    "        def train(self,X , y, epochs = 5000):\n",
    "\n",
    "            for epoch in epochs:\n",
    "\n",
    "                hidden_input = np.dot(X , self.weights_input) + self.bias_hidden\n",
    "                hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "                final_input = np.dot(hidden_output, self.weights_output) + self.bias_output\n",
    "                final_output = sigmoid(final_input)\n",
    "\n",
    "                final_error = error(y - final_output)\n",
    "\n",
    "                d_output = final_error * self.sigmoid_derivative(final_output)\n",
    "                d_hidden = d_output.dot(self.weights_output.T) * sigmoid_derivative(hidden_output)\n",
    "\n",
    "                self.weights_output = hidden_output.T.dot(d_output) * self.learning_rate\n",
    "                self.bias_output += np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "                self.weights_input += X.T.dot(d_hidden) * self.learning_rate\n",
    "                self.bias_hidden += np.sum(d_hidden, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "                if epoch % 1000 == 0:\n",
    "                        \n",
    "                        loss = np.mean(np.square(error))\n",
    "                        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "        \n",
    "        def predict_proba(self, X):\n",
    "            _, hidden_output, _, final_output = self.forward(X)\n",
    "            return final_output  # probabilities in (0,1)\n",
    "\n",
    "        def predict(self, X, threshold=0.5):\n",
    "            probs = self.predict_proba(X)\n",
    "            return (probs >= threshold).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd51c093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.344741\n",
      "Epoch 1000, Loss: 0.090373\n",
      "Epoch 2000, Loss: 0.016695\n",
      "Epoch 3000, Loss: 0.006578\n",
      "Epoch 4000, Loss: 0.003774\n",
      "Probabilities:\n",
      " [[0.05428218]\n",
      " [0.05406546]\n",
      " [0.93428731]\n",
      " [0.00846542]]\n",
      "Predicted labels: [0 0 1 0]\n",
      "Predict [1,1]: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np \n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "data = [\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [0, 0]\n",
    "]\n",
    "labels = [0, 0, 1, 0]  \n",
    "\n",
    "data = np.array(data, dtype=float)\n",
    "labels = np.array(labels, dtype=float).reshape(-1, 1)\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.weights_input = np.random.rand(2, 2)\n",
    "        self.weights_output = np.random.rand(2, 1)\n",
    "        # Recommended: zero biases for stable start\n",
    "        self.bias_hidden = np.zeros((1, 2))\n",
    "        self.bias_output = np.zeros((1, 1))\n",
    "        self.learning_rate = 0.1\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "    def sigmoid_derivative(self, a):\n",
    "        return a * (1 - a)\n",
    "        \n",
    "    def error(self, y, y_p):\n",
    "        return y - y_p\n",
    "\n",
    "    def forward(self, X):\n",
    "        hidden_input = np.dot(X, self.weights_input) + self.bias_hidden\n",
    "        hidden_output = self.sigmoid(hidden_input)\n",
    "        final_input = np.dot(hidden_output, self.weights_output) + self.bias_output\n",
    "        final_output = self.sigmoid(final_input)\n",
    "        return hidden_input, hidden_output, final_input, final_output\n",
    "        \n",
    "    def train(self, X, y, epochs=5000):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward\n",
    "            hidden_input = np.dot(X, self.weights_input) + self.bias_hidden\n",
    "            hidden_output = self.sigmoid(hidden_input)\n",
    "\n",
    "            final_input = np.dot(hidden_output, self.weights_output) + self.bias_output\n",
    "            final_output = self.sigmoid(final_input)\n",
    "\n",
    "            # Error and loss\n",
    "            final_error = self.error(y, final_output)\n",
    "            loss = np.mean(final_error ** 2)\n",
    "\n",
    "            # Backprop\n",
    "            d_output = final_error * self.sigmoid_derivative(final_output)\n",
    "            d_hidden = np.dot(d_output, self.weights_output.T) * self.sigmoid_derivative(hidden_output)\n",
    "\n",
    "            # Gradient descent updates\n",
    "            self.weights_output += self.learning_rate * np.dot(hidden_output.T, d_output)\n",
    "            self.bias_output   += self.learning_rate * np.sum(d_output, axis=0, keepdims=True)\n",
    "\n",
    "            self.weights_input += self.learning_rate * np.dot(X.T, d_hidden)\n",
    "            self.bias_hidden   += self.learning_rate * np.sum(d_hidden, axis=0, keepdims=True)\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        _, _, _, final_output = self.forward(X)\n",
    "        return final_output  # probabilities in (0,1)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probs = self.predict_proba(X)\n",
    "        return (probs >= threshold).astype(int)\n",
    "\n",
    "# ------- Run training and test prediction -------\n",
    "if __name__ == \"__main__\":\n",
    "    nn = SimpleNeuralNetwork()\n",
    "    nn.train(data, labels, epochs=5000)\n",
    "\n",
    "    print(\"Probabilities:\\n\", nn.predict_proba(data))\n",
    "    print(\"Predicted labels:\", nn.predict(data).ravel())\n",
    "\n",
    "    # Test a single input: AND(1,1) -> 1\n",
    "    print(\"Predict [1,1]:\", nn.predict(np.array([[1., 1.]])).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c5bb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2662\n",
      "Epoch 1000, Loss: 0.1434\n",
      "Epoch 2000, Loss: 0.0228\n",
      "Epoch 3000, Loss: 0.0069\n",
      "Epoch 4000, Loss: 0.0036\n",
      "\n",
      "Predictions after training:\n",
      "[0 0] -> 0.004 (Target: 0)\n",
      "[0 1] -> 0.050 (Target: 0)\n",
      "[1 0] -> 0.047 (Target: 0)\n",
      "[1 1] -> 0.933 (Target: 1)\n"
     ]
    }
   ],
   "source": [
    "# Here is a “student exam pass/fail” scenario with full code and simple explanations.\n",
    "\n",
    "# Scenario: Will a student pass the exam?\n",
    "# Inputs (2 features):\n",
    "\n",
    "# Study hours (0 = didn’t study, 1 = studied enough)\n",
    "\n",
    "# Class attendance (0 = poor, 1 = good)\n",
    "\n",
    "# Output:\n",
    "\n",
    "# 1 = passes exam\n",
    "\n",
    "# 0 = fails exam\n",
    "\n",
    "# Rule (hidden from model, but intuitive):\n",
    "\n",
    "# Student usually passes if both study and attendance are good.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1. Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "# 2. Dataset: [study_hours, attendance]\n",
    "# 0 = low, 1 = high/good\n",
    "X = np.array([\n",
    "    [0, 0],   # didn't study, didn't attend\n",
    "    [0, 1],   # didn't study, attended class\n",
    "    [1, 0],   # studied, but didn't attend\n",
    "    [1, 1]    # studied and attended\n",
    "])\n",
    "\n",
    "# y: did the student pass? (0 = fail, 1 = pass)\n",
    "y = np.array([\n",
    "    [0],  # low study, low attendance -> fail\n",
    "    [0],  # low study, good attendance -> likely fail\n",
    "    [0],  # good study, low attendance -> risky, mark fail\n",
    "    [1]   # good study, good attendance -> pass\n",
    "])\n",
    "\n",
    "\n",
    "# 3. Initialize weights and biases\n",
    "np.random.seed(1)\n",
    "weights_input_hidden = np.random.rand(2, 2)   # 2 inputs -> 2 hidden neurons\n",
    "weights_hidden_output = np.random.rand(2, 1)  # 2 hidden -> 1 output neuron\n",
    "bias_hidden = np.zeros((1, 2))\n",
    "bias_output = np.zeros((1, 1))\n",
    "\n",
    "# Training settings\n",
    "learning_rate = 0.1\n",
    "epochs = 5000\n",
    "\n",
    "\n",
    "# 4. Training loop\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # ---- Forward pass ----\n",
    "    # Hidden layer\n",
    "    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "    # Output layer\n",
    "    final_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "    final_output = sigmoid(final_input)\n",
    "\n",
    "    # ---- Error ----\n",
    "    error = y - final_output\n",
    "\n",
    "    # ---- Backpropagation ----\n",
    "    d_output = error * sigmoid_derivative(final_output)\n",
    "    d_hidden = d_output.dot(weights_hidden_output.T) * sigmoid_derivative(hidden_output)\n",
    "\n",
    "    # ---- Update weights and biases ----\n",
    "    weights_hidden_output += hidden_output.T.dot(d_output) * learning_rate\n",
    "    bias_output += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    weights_input_hidden += X.T.dot(d_hidden) * learning_rate\n",
    "    bias_hidden += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    # Print loss sometimes\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "# 5. Predictions after training\n",
    "print(\"\\nPredictions after training:\")\n",
    "for i, inputs in enumerate(X):\n",
    "    hidden_output = sigmoid(np.dot(inputs, weights_input_hidden) + bias_hidden)\n",
    "    final_output = sigmoid(np.dot(hidden_output, weights_hidden_output) + bias_output)\n",
    "    print(f\"{inputs} -> {final_output[0][0]:.3f} (Target: {y[i][0]})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff875551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DevEnv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
